# -*- coding: utf-8 -*-
"""fake_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ke1nHa9UjTR1Pf9jqu1RTTftyhrc0YQZ
"""

import pandas as pd

# Read the fake news dataset
fake_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/fake.csv')

# Read the true news dataset
true_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/true.csv')

# Display the first few rows of each dataset
print("Fake News Dataset:")
print(fake_news_df.head())

print("\nTrue News Dataset:")
print(true_news_df.head())

import pandas as pd


fake_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/fake.csv')

# Read the true news dataset
true_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/true.csv')
# Add a 'label' column to distinguish between fake and true news
fake_news_df['label'] = 0  # Label fake news as 0
true_news_df['label'] = 1  # Label true news as 1

# Combine the datasets
combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)

# Shuffle the combined dataset
combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Display the first few rows of the combined dataset
print("Combined Dataset:")
print(combined_df.head())

# Save the combined dataset to a new CSV file (optional)
combined_df.to_csv('combined_news_dataset.csv', index=False)

print(combined_df.isnull().sum())
print(combined_df['label'].value_counts())

combined_df['subject'].value_counts().plot(kind='bar', title='Subject Distribution')

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt_tab')
# Download NLTK data (run this once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Preprocess function
def preprocess_text(text):
    # Remove punctuation, special characters, and numbers
    text = re.sub(r'[^a-zA-Z]', ' ', text)

    # Convert text to lowercase
    text = text.lower()

    # Tokenize the text
    words = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join words back to form a cleaned string
    return ' '.join(words)

# Example: Apply preprocessing to the 'text' column
combined_df['cleaned_text'] = combined_df['text'].apply(preprocess_text)

# Display the first few rows of the cleaned dataset
print("Cleaned Dataset:")
print(combined_df[['text', 'cleaned_text']].head())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Feature extraction using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 features
X = tfidf_vectorizer.fit_transform(combined_df['cleaned_text']).toarray()

# Target variable (labels)
y = combined_df['label']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import joblib
joblib.dump(rf_classifier, 'fake_news_detector_model.pkl')
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')

print(time_series_data['True_News'].describe())
print(time_series_data['True_News'].value_counts())

print(combined_df['label'].unique())

print(combined_df['label'].value_counts())

# Read the true news dataset
combined_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/true.csv')

print(combined_df)

import pandas as pd

# Read the fake news dataset
fake_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/fake.csv')

# Read the true news dataset
true_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/true.csv')

# Add the label column
true_news_df['label'] = 1  # Label true news as 1
fake_news_df['label'] = 0  # Label fake news as 0

# Combine the datasets
combined_df = pd.concat([true_news_df, fake_news_df], ignore_index=True)

# Save the combined dataset (optional)
combined_df.to_csv('/content/drive/MyDrive/fakenewsdetection/combined_news.csv', index=False)

# Display the first few rows of the combined dataset
print(combined_df.head())

print("Label Counts in combined_df:")
print(combined_df['label'].value_counts())
fake_news = combined_df[combined_df['label'] == 0]
print("Sample Fake News Data:")
print(fake_news.head())
print("Fake News Date Column Summary:")
print(fake_news['date'].describe())
print("Grouped Data for Label=0 (Fake News):")
print(grouped_data[grouped_data['label'] == 0])

import pandas as pd

# Load the datasets
true_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/true.csv')
fake_news_df = pd.read_csv('/content/drive/MyDrive/fakenewsdetection/fakenews.zip (Unzipped Files)/fake.csv')

# Add the label column
true_news_df['label'] = 1  # Label true news as 1
fake_news_df['label'] = 0  # Label fake news as 0

# Ensure the 'date' column is in datetime format for both datasets
true_news_df['date'] = pd.to_datetime(true_news_df['date'], errors='coerce')
fake_news_df['date'] = pd.to_datetime(fake_news_df['date'], errors='coerce')

# Drop rows with invalid dates in both datasets
true_news_df = true_news_df.dropna(subset=['date'])
fake_news_df = fake_news_df.dropna(subset=['date'])

# Combine the datasets
combined_df = pd.concat([true_news_df, fake_news_df], ignore_index=True)

# Extract the year-month for aggregation
combined_df['year_month'] = combined_df['date'].dt.to_period('M')

# Verify the combined dataset
print("Label Counts in Combined Dataset:")
print(combined_df['label'].value_counts())

# Group by year-month and label (fake or true) to get article counts
grouped_data = combined_df.groupby(['year_month', 'label']).size().reset_index(name='count')

# Verify grouped data for label=0 and label=1
print("\nGrouped Data for Label=0 (Fake News):")
print(grouped_data[grouped_data['label'] == 0])

print("\nGrouped Data for Label=1 (True News):")
print(grouped_data[grouped_data['label'] == 1])

# Pivot the data to create columns for Fake_News and True_News
time_series_data = grouped_data.pivot(index='year_month', columns='label', values='count').fillna(0)
time_series_data.columns = ['Fake_News', 'True_News']

# Verify the pivoted data
print("\nPivoted Time Series Data:")
print(time_series_data.head())

# Convert the index to datetime for proper time series indexing
time_series_data.index = time_series_data.index.to_timestamp()

# Plot the time series for Fake and True News
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(time_series_data['Fake_News'], label="Fake News", color='blue')
plt.plot(time_series_data['True_News'], label="True News", color='orange')
plt.title("Volume of Fake and True News Over Time")
plt.xlabel("Year-Month")
plt.ylabel("Number of Articles")
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Plot the time series for Fake and True News with enhancements
plt.figure(figsize=(14, 8))

# Plot Fake News and True News trends
plt.plot(
    time_series_data.index,
    time_series_data['Fake_News'],
    label="Fake News",
    color='blue',
    linewidth=2
)
plt.plot(
    time_series_data.index,
    time_series_data['True_News'],
    label="True News",
    color='orange',
    linewidth=2
)

# Add titles and labels
plt.title("Time Series of Fake and True News Articles", fontsize=16, fontweight='bold')
plt.xlabel("Year-Month", fontsize=14)
plt.ylabel("Number of Articles", fontsize=14)

# Add gridlines for better readability
plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)

# Customize the x-axis ticks
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)

# Add a legend with better positioning
plt.legend(fontsize=12, loc='upper left')

# Add annotations for peak points
max_fake_news = time_series_data['Fake_News'].max()
max_fake_date = time_series_data['Fake_News'].idxmax()
plt.annotate(
    f"Peak Fake News: {int(max_fake_news)}",
    xy=(max_fake_date, max_fake_news),
    xytext=(max_fake_date, max_fake_news + 500),
    arrowprops=dict(facecolor='black', arrowstyle="->"),
    fontsize=12
)

max_true_news = time_series_data['True_News'].max()
max_true_date = time_series_data['True_News'].idxmax()
plt.annotate(
    f"Peak True News: {int(max_true_news)}",
    xy=(max_true_date, max_true_news),
    xytext=(max_true_date, max_true_news - 1000),
    arrowprops=dict(facecolor='black', arrowstyle="->"),
    fontsize=12
)

# Show the plot
plt.tight_layout()
plt.show()

